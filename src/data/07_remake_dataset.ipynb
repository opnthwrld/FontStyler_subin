{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remake Dataset\n",
    "지금까지는 카테고리별 obj 파일을 생성했다.<br/>\n",
    "학습의 용이성을 위해 카테고리를 합친 형태의 파일을 생성할 것이다.\n",
    "\n",
    "---\n",
    "\n",
    "### Split 비율\n",
    "train : val : test = 0.8 : 0.15 : 0.05 <br/>\n",
    "(train.py의 코드를 참고할 것이다.)\n",
    "\n",
    "### 사용할 카테고리\n",
    "1. Serif (182개)\n",
    "2. Display (283개)\n",
    "3. Handwriting (140개) <br/>\n",
    "→ 각 비율에 맞춰 train.obj, val.obj, test.obj 생성\n",
    "\n",
    "### 기타\n",
    "각 obj 파일은 카테고리 순으로 저장된다(단, 카테고리 내 font는 random).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle as pickle\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from common.dataset import FontDataset\n",
    "from common.dataset import NewFontDataset\n",
    "from common.dataset import PickledImageProvider\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = .15\n",
    "test_split       = .05\n",
    "shuffle_dataset  = True\n",
    "random_seed      = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 카테고리마다 이미지를 비율에 맞춰 split한다.\n",
    "- Random하게 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filename(path):\n",
    "    filenames = []\n",
    "    for filename in glob.iglob(path + '*.png'):\n",
    "        filenames.append(filename[17:])\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 이미지 파일명\n",
    "original_serif       = extract_filename('collection/img/0/')\n",
    "original_display     = extract_filename('collection/img/2/')\n",
    "original_handwriting = extract_filename('collection/img/3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리별 파일 개수\n",
    "size_serif       = len(original_serif)       # 182 * 52 = 9464\n",
    "size_display     = len(original_display)     # 283 * 52 = 14716\n",
    "size_handwriting = len(original_handwriting) # 140 * 52 = 7280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_each_font(size, splt_test, splt_val):\n",
    "    test = int(np.floor(splt_test * size))\n",
    "    val  = int(np.floor((splt_test + splt_val) * size))\n",
    "    return test, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serif split\n",
    "split_test_serif, split_valid_serif = split_each_font(size_serif, test_split, validation_split)\n",
    "\n",
    "# display split\n",
    "split_test_display, split_valid_display = split_each_font(size_display, test_split, validation_split)\n",
    "\n",
    "# serif split\n",
    "split_test_handwriting, split_valid_handwriting = split_each_font(size_handwriting, test_split, validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---serif---\n",
      "test:   473\n",
      "valid+test:  1892\n",
      "---display---\n",
      "test:   735\n",
      "valid+test:  2943\n",
      "---handwriting---\n",
      "test:   364\n",
      "valid+test:  1456\n"
     ]
    }
   ],
   "source": [
    "print('---serif---')\n",
    "print('test:  ', split_test_serif)\n",
    "print('valid+test: ', split_valid_serif)\n",
    "\n",
    "print('---display---')\n",
    "print('test:  ', split_test_display)\n",
    "print('valid+test: ', split_valid_display)\n",
    "\n",
    "print('---handwriting---')\n",
    "print('test:  ', split_test_handwriting)\n",
    "print('valid+test: ', split_valid_handwriting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 폰트별로 split하여 파일 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_sampler(size, splt_test, splt_val):\n",
    "    idx = list(range(size))\n",
    "    train_idxs = idx[splt_val:]\n",
    "    val_idxs   = idx[splt_test: splt_val]\n",
    "    test_idxs  = idx[: splt_test]\n",
    "    return train_idxs, val_idxs, test_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_i_serif, val_i_serif, test_i_serif = get_idx_sampler(size_serif, split_test_serif, split_valid_serif)\n",
    "train_i_disp, val_i_disp, test_i_disp    = get_idx_sampler(size_display, split_test_display, split_valid_display)\n",
    "train_i_hand, val_i_hand, test_i_hand    = get_idx_sampler(size_handwriting, split_test_handwriting, split_valid_handwriting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              train val test\n",
      "serif:        7572 1419 473\n",
      "display:      11773 2208 735\n",
      "handwriting:  5824 1092 364\n"
     ]
    }
   ],
   "source": [
    "print('              train val test')\n",
    "print('serif:       ', len(train_i_serif), len(val_i_serif), len(test_i_serif))\n",
    "print('display:     ', len(train_i_disp), len(val_i_disp), len(test_i_disp))\n",
    "print('handwriting: ', len(train_i_hand), len(val_i_hand), len(test_i_hand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "기존 train.obj 파일에서는 폰트들이 랜덤하게 저장되어 있음을 확인했다. <br/>\n",
    "그렇다면 그냥 일정 개수만큼의 pickle을 불러온 후 따로 저장하면 되지 않을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './dataset/integrated/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1000 examples\n",
      "processed 2000 examples\n",
      "processed 3000 examples\n",
      "processed 4000 examples\n",
      "processed 5000 examples\n",
      "processed 6000 examples\n",
      "processed 7000 examples\n",
      "processed 8000 examples\n",
      "processed 9000 examples\n",
      "processed 10000 examples\n",
      "processed 11000 examples\n",
      "processed 12000 examples\n",
      "processed 13000 examples\n",
      "processed 14000 examples\n",
      "processed 15000 examples\n",
      "processed 16000 examples\n",
      "processed 17000 examples\n",
      "processed 18000 examples\n",
      "processed 19000 examples\n",
      "processed 20000 examples\n",
      "processed 21000 examples\n",
      "processed 22000 examples\n",
      "processed 23000 examples\n",
      "processed 24000 examples\n",
      "processed 25000 examples\n",
      "processed 26000 examples\n",
      "processed 27000 examples\n",
      "processed 28000 examples\n",
      "processed 29000 examples\n",
      "processed 30000 examples\n",
      "processed 31000 examples\n",
      "processed 32000 examples\n",
      "processed 33000 examples\n",
      "processed 34000 examples\n",
      "processed 35000 examples\n",
      "processed 36000 examples\n",
      "processed 37000 examples\n",
      "processed 38000 examples\n",
      "processed 39000 examples\n",
      "processed 40000 examples\n",
      "processed 41000 examples\n",
      "processed 42000 examples\n",
      "processed 43000 examples\n",
      "processed 44000 examples\n",
      "processed 45000 examples\n",
      "processed 46000 examples\n",
      "processed 47000 examples\n",
      "unpickled total 47320 examples\n",
      "saved total 9464 examples only for byte\n",
      "processed 1000 examples\n",
      "processed 2000 examples\n",
      "processed 3000 examples\n",
      "processed 4000 examples\n",
      "processed 5000 examples\n",
      "processed 6000 examples\n",
      "processed 7000 examples\n",
      "processed 8000 examples\n",
      "processed 9000 examples\n",
      "processed 10000 examples\n",
      "processed 11000 examples\n",
      "processed 12000 examples\n",
      "processed 13000 examples\n",
      "processed 14000 examples\n",
      "processed 15000 examples\n",
      "processed 16000 examples\n",
      "processed 17000 examples\n",
      "processed 18000 examples\n",
      "processed 19000 examples\n",
      "processed 20000 examples\n",
      "processed 21000 examples\n",
      "processed 22000 examples\n",
      "processed 23000 examples\n",
      "processed 24000 examples\n",
      "processed 25000 examples\n",
      "processed 26000 examples\n",
      "processed 27000 examples\n",
      "processed 28000 examples\n",
      "processed 29000 examples\n",
      "processed 30000 examples\n",
      "processed 31000 examples\n",
      "processed 32000 examples\n",
      "processed 33000 examples\n",
      "processed 34000 examples\n",
      "processed 35000 examples\n",
      "processed 36000 examples\n",
      "processed 37000 examples\n",
      "processed 38000 examples\n",
      "processed 39000 examples\n",
      "processed 40000 examples\n",
      "processed 41000 examples\n",
      "processed 42000 examples\n",
      "processed 43000 examples\n",
      "processed 44000 examples\n",
      "processed 45000 examples\n",
      "processed 46000 examples\n",
      "processed 47000 examples\n",
      "processed 48000 examples\n",
      "processed 49000 examples\n",
      "processed 50000 examples\n",
      "processed 51000 examples\n",
      "processed 52000 examples\n",
      "processed 53000 examples\n",
      "processed 54000 examples\n",
      "processed 55000 examples\n",
      "processed 56000 examples\n",
      "processed 57000 examples\n",
      "processed 58000 examples\n",
      "processed 59000 examples\n",
      "processed 60000 examples\n",
      "processed 61000 examples\n",
      "processed 62000 examples\n",
      "processed 63000 examples\n",
      "processed 64000 examples\n",
      "processed 65000 examples\n",
      "processed 66000 examples\n",
      "processed 67000 examples\n",
      "processed 68000 examples\n",
      "processed 69000 examples\n",
      "processed 70000 examples\n",
      "processed 71000 examples\n",
      "processed 72000 examples\n",
      "processed 73000 examples\n",
      "unpickled total 73580 examples\n",
      "saved total 14716 examples only for byte\n",
      "processed 1000 examples\n",
      "processed 2000 examples\n",
      "processed 3000 examples\n",
      "processed 4000 examples\n",
      "processed 5000 examples\n",
      "processed 6000 examples\n",
      "processed 7000 examples\n",
      "processed 8000 examples\n",
      "processed 9000 examples\n",
      "processed 10000 examples\n",
      "processed 11000 examples\n",
      "processed 12000 examples\n",
      "processed 13000 examples\n",
      "processed 14000 examples\n",
      "processed 15000 examples\n",
      "processed 16000 examples\n",
      "processed 17000 examples\n",
      "processed 18000 examples\n",
      "processed 19000 examples\n",
      "processed 20000 examples\n",
      "processed 21000 examples\n",
      "processed 22000 examples\n",
      "processed 23000 examples\n",
      "processed 24000 examples\n",
      "processed 25000 examples\n",
      "processed 26000 examples\n",
      "processed 27000 examples\n",
      "processed 28000 examples\n",
      "processed 29000 examples\n",
      "processed 30000 examples\n",
      "processed 31000 examples\n",
      "processed 32000 examples\n",
      "processed 33000 examples\n",
      "processed 34000 examples\n",
      "processed 35000 examples\n",
      "processed 36000 examples\n",
      "unpickled total 36400 examples\n",
      "saved total 7280 examples only for byte\n"
     ]
    }
   ],
   "source": [
    "dataset_serif = FontDataset(PickledImageProvider(data_dir+'train_0.obj'))\n",
    "dataset_disp  = FontDataset(PickledImageProvider(data_dir+'train_2.obj'))\n",
    "dataset_hand  = FontDataset(PickledImageProvider(data_dir+'train_3.obj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './dataset/allfonts/'\n",
    "\n",
    "dict_train = {\n",
    "    'filename': 'train.obj',\n",
    "    'serif': len(train_i_serif),\n",
    "    'display': len(train_i_disp),\n",
    "    'handwriting': len(train_i_hand)\n",
    "}\n",
    "\n",
    "dict_val = {\n",
    "    'filename': 'val.obj',\n",
    "    'serif': len(val_i_serif),\n",
    "    'display': len(val_i_disp),\n",
    "    'handwriting': len(val_i_hand)\n",
    "}\n",
    "\n",
    "dict_test = {\n",
    "    'filename': 'test.obj',\n",
    "    'serif': len(test_i_serif),\n",
    "    'display': len(test_i_disp),\n",
    "    'handwriting': len(test_i_hand)\n",
    "}\n",
    "\n",
    "dsets = [dataset_serif, dataset_disp, dataset_hand]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25169 pickles saved in train.obj\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(save_dir, dict_train['filename'])\n",
    "\n",
    "with open(train_path, 'wb') as ft:\n",
    "    count = 0\n",
    "    for dset in dsets: # for serif, disp, hand\n",
    "        category = np.argmax(dset[0][0]['category_vector'], axis=0) # one-hot to Integer\n",
    "        \n",
    "        idx = -1\n",
    "        if category == 0:\n",
    "            idx = dict_train['serif']\n",
    "        elif category == 2:\n",
    "            idx = dict_train['display']\n",
    "        elif category == 3:\n",
    "            idx = dict_train['handwriting']\n",
    "        \n",
    "        for (i, dd) in enumerate(dset): # for each data\n",
    "            if i < idx:\n",
    "                torch.save(dd, ft)\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "    print('{} pickles saved in train.obj'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### val.obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4719 pickles saved in val.obj\n"
     ]
    }
   ],
   "source": [
    "val_path = os.path.join(save_dir, dict_val['filename'])\n",
    "\n",
    "with open(val_path, 'wb') as ft:\n",
    "    count = 0\n",
    "    for dset in dsets: # for serif, disp, hand\n",
    "        category = np.argmax(dset[0][0]['category_vector'], axis=0) # one-hot to Integer\n",
    "        \n",
    "        before, idx = -1, -1\n",
    "        if category == 0:\n",
    "            before = dict_train['serif']\n",
    "            idx = dict_val['serif']\n",
    "        elif category == 2:\n",
    "            before = dict_train['display']\n",
    "            idx = dict_val['display']\n",
    "        elif category == 3:\n",
    "            before = dict_train['handwriting']\n",
    "            idx = dict_val['handwriting']\n",
    "        \n",
    "        for (i, dd) in enumerate(dset): # for each data\n",
    "            if i < before:\n",
    "                continue\n",
    "            elif i < before + idx:\n",
    "                torch.save(dd, ft)\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "    print('{} pickles saved in val.obj'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test.obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1572 pickles saved in test.obj\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(save_dir, dict_test['filename'])\n",
    "\n",
    "with open(test_path, 'wb') as ft:\n",
    "    count = 0\n",
    "    for dset in dsets: # for serif, disp, hand\n",
    "        category = np.argmax(dset[0][0]['category_vector'], axis=0) # one-hot to Integer\n",
    "        \n",
    "        before = -1\n",
    "        if category == 0:\n",
    "            before = dict_train['serif'] + dict_val['serif']\n",
    "        elif category == 2:\n",
    "            before = dict_train['display'] + dict_val['display']\n",
    "        elif category == 3:\n",
    "            before = dict_train['handwriting'] + dict_val['handwriting']\n",
    "        \n",
    "        for (i, dd) in enumerate(dset): # for each data\n",
    "            if i >= before:\n",
    "                torch.save(dd, ft)\n",
    "                count += 1\n",
    "\n",
    "    print('{} pickles saved in test.obj'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir = './dataset/allfonts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1000 examples\n",
      "processed 2000 examples\n",
      "processed 3000 examples\n",
      "processed 4000 examples\n",
      "processed 5000 examples\n",
      "processed 6000 examples\n",
      "processed 7000 examples\n",
      "processed 8000 examples\n",
      "processed 9000 examples\n",
      "processed 10000 examples\n",
      "processed 11000 examples\n",
      "processed 12000 examples\n",
      "processed 13000 examples\n",
      "processed 14000 examples\n",
      "processed 15000 examples\n",
      "processed 16000 examples\n",
      "processed 17000 examples\n",
      "processed 18000 examples\n",
      "processed 19000 examples\n",
      "processed 20000 examples\n",
      "processed 21000 examples\n",
      "processed 22000 examples\n",
      "processed 23000 examples\n",
      "processed 24000 examples\n",
      "processed 25000 examples\n",
      "processed 26000 examples\n",
      "processed 27000 examples\n",
      "processed 28000 examples\n",
      "processed 29000 examples\n",
      "processed 30000 examples\n",
      "processed 31000 examples\n",
      "processed 32000 examples\n",
      "processed 33000 examples\n",
      "processed 34000 examples\n",
      "processed 35000 examples\n",
      "processed 36000 examples\n",
      "processed 37000 examples\n",
      "processed 38000 examples\n",
      "processed 39000 examples\n",
      "processed 40000 examples\n",
      "processed 41000 examples\n",
      "processed 42000 examples\n",
      "processed 43000 examples\n",
      "processed 44000 examples\n",
      "processed 45000 examples\n",
      "processed 46000 examples\n",
      "processed 47000 examples\n",
      "processed 48000 examples\n",
      "processed 49000 examples\n",
      "processed 50000 examples\n",
      "processed 51000 examples\n",
      "processed 52000 examples\n",
      "processed 53000 examples\n",
      "processed 54000 examples\n",
      "processed 55000 examples\n",
      "processed 56000 examples\n",
      "processed 57000 examples\n",
      "processed 58000 examples\n",
      "processed 59000 examples\n",
      "processed 60000 examples\n",
      "processed 61000 examples\n",
      "processed 62000 examples\n",
      "processed 63000 examples\n",
      "processed 64000 examples\n",
      "processed 65000 examples\n",
      "processed 66000 examples\n",
      "processed 67000 examples\n",
      "processed 68000 examples\n",
      "processed 69000 examples\n",
      "processed 70000 examples\n",
      "processed 71000 examples\n",
      "processed 72000 examples\n",
      "processed 73000 examples\n",
      "processed 74000 examples\n",
      "processed 75000 examples\n",
      "processed 76000 examples\n",
      "processed 77000 examples\n",
      "processed 78000 examples\n",
      "processed 79000 examples\n",
      "processed 80000 examples\n",
      "processed 81000 examples\n",
      "processed 82000 examples\n",
      "processed 83000 examples\n",
      "processed 84000 examples\n",
      "processed 85000 examples\n",
      "processed 86000 examples\n",
      "processed 87000 examples\n",
      "processed 88000 examples\n",
      "processed 89000 examples\n",
      "processed 90000 examples\n",
      "processed 91000 examples\n",
      "processed 92000 examples\n",
      "processed 93000 examples\n",
      "processed 94000 examples\n",
      "processed 95000 examples\n",
      "processed 96000 examples\n",
      "processed 97000 examples\n",
      "processed 98000 examples\n",
      "processed 99000 examples\n",
      "processed 100000 examples\n",
      "processed 101000 examples\n",
      "processed 102000 examples\n",
      "processed 103000 examples\n",
      "processed 104000 examples\n",
      "processed 105000 examples\n",
      "processed 106000 examples\n",
      "processed 107000 examples\n",
      "processed 108000 examples\n",
      "processed 109000 examples\n",
      "processed 110000 examples\n",
      "processed 111000 examples\n",
      "processed 112000 examples\n",
      "processed 113000 examples\n",
      "processed 114000 examples\n",
      "processed 115000 examples\n",
      "processed 116000 examples\n",
      "processed 117000 examples\n",
      "processed 118000 examples\n",
      "processed 119000 examples\n",
      "processed 120000 examples\n",
      "processed 121000 examples\n",
      "processed 122000 examples\n",
      "processed 123000 examples\n",
      "processed 124000 examples\n",
      "processed 125000 examples\n",
      "unpickled total 125845 examples\n",
      "saved total 25169 examples only for byte\n",
      "processed 1000 examples\n",
      "processed 2000 examples\n",
      "processed 3000 examples\n",
      "processed 4000 examples\n",
      "processed 5000 examples\n",
      "processed 6000 examples\n",
      "processed 7000 examples\n",
      "processed 8000 examples\n",
      "processed 9000 examples\n",
      "processed 10000 examples\n",
      "processed 11000 examples\n",
      "processed 12000 examples\n",
      "processed 13000 examples\n",
      "processed 14000 examples\n",
      "processed 15000 examples\n",
      "processed 16000 examples\n",
      "processed 17000 examples\n",
      "processed 18000 examples\n",
      "processed 19000 examples\n",
      "processed 20000 examples\n",
      "processed 21000 examples\n",
      "processed 22000 examples\n",
      "processed 23000 examples\n",
      "unpickled total 23595 examples\n",
      "saved total 4719 examples only for byte\n",
      "processed 1000 examples\n",
      "processed 2000 examples\n",
      "processed 3000 examples\n",
      "processed 4000 examples\n",
      "processed 5000 examples\n",
      "processed 6000 examples\n",
      "processed 7000 examples\n",
      "unpickled total 7860 examples\n",
      "saved total 1572 examples only for byte\n"
     ]
    }
   ],
   "source": [
    "new_train = NewFontDataset(PickledImageProvider(new_dir+'train.obj'))\n",
    "new_val   = NewFontDataset(PickledImageProvider(new_dir+'val.obj'))\n",
    "new_test  = NewFontDataset(PickledImageProvider(new_dir+'test.obj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25169, 4719, 1572)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train), len(new_val), len(new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Test 카테고리 순서 확인\n",
    "\n",
    "#### 1. index 0 ~ 472 → **SERIF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'category_vector': array([1, 0, 0, 0, 0]),\n",
       "  'font': 26,\n",
       "  'alphabet_vector': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]]),\n",
       " {'category_vector': 5, 'alphabet_vector': 52, 'font_vector': 16384})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test[472] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'category_vector': array([0, 0, 1, 0, 0]),\n",
       "  'font': 248,\n",
       "  'alphabet_vector': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]]),\n",
       " {'category_vector': 5, 'alphabet_vector': 52, 'font_vector': 16384})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test[473]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. index 473 ~ 1207 → **DISPLAY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'category_vector': array([0, 0, 1, 0, 0]),\n",
       "  'font': 277,\n",
       "  'alphabet_vector': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]]),\n",
       " {'category_vector': 5, 'alphabet_vector': 52, 'font_vector': 16384})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test[1207]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'category_vector': array([0, 0, 0, 1, 0]),\n",
       "  'font': 0,\n",
       "  'alphabet_vector': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]]),\n",
       " {'category_vector': 5, 'alphabet_vector': 52, 'font_vector': 16384})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test[1208]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. index 1208 ~  → **HANDWRITING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'category_vector': array([0, 0, 0, 1, 0]),\n",
       "  'font': 63,\n",
       "  'alphabet_vector': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]]),\n",
       " {'category_vector': 5, 'alphabet_vector': 52, 'font_vector': 16384})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test[1571]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 정리\n",
    "train / val / test == 0.8 / 0.15 / 0.05 <br/>\n",
    "새로운 데이터 경로: dataset/allfonts/\n",
    "\n",
    "## train.obj\n",
    "- serif: 7572 개\n",
    "- display: 11773 개\n",
    "- handwriting: 5824 개\n",
    "\n",
    "## val.obj\n",
    "- serif: 1419 개\n",
    "- display: 2208 개\n",
    "- handwriting: 1092 개\n",
    "\n",
    "## test.obj\n",
    "- serif: 473 개\n",
    "- display: 735 개\n",
    "- handwriting: 364 개\n",
    "\n",
    "\n",
    "### 참고\n",
    "- train/val/test 모든 파일은 카테고리별로 indexing이 되어 있다.\n",
    "- 파일의 용량이 커서 github에는 압축 파일만 올린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
