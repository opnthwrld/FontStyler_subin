{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial AE\n",
    "\n",
    "#### Adversarial ae라고 naming 하기는 하였으나, 기존에 나와있는 vae를 이용한 구조는 아니고,\n",
    "#### 정민정님이 사용한것과 같은 형태의 AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr as py # library to read .Rdata files in python\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix,confusion_matrix,classification_report\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV,KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,f1_score,log_loss,recall_score,classification_report\n",
    "from sklearn.preprocessing import scale, robust_scale, minmax_scale, maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset  \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.model import AE_base\n",
    "from src.data.common.dataset import FontDataset, PickledImageProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "validation_split = .15\n",
    "test_split = .05\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "lr = 0.0001\n",
    "\n",
    "log_interval = 10\n",
    "epochs = 200\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.function import conv2d, deconv2d, batch_norm, lrelu, dropout\n",
    "from src.data.common.dataset import NewFontDataset, PickledImageProvider, KoreanFontDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Dataset\n",
    "data_dir = 'src/data/dataset/kor/'\n",
    "train_set = KoreanFontDataset(PickledImageProvider(data_dir+'train.obj'), vector_size=20)\n",
    "valid_set = KoreanFontDataset(PickledImageProvider(data_dir+'val.obj'), vector_size=20)\n",
    "test_set = KoreanFontDataset(PickledImageProvider(data_dir+'test.obj'), vector_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get idx samplers\n",
    "train_set_size = len(train_set)\n",
    "valid_set_size = len(valid_set)\n",
    "train_idxs = list(range(train_set_size))\n",
    "valid_idxs = list(range(valid_set_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(train_idxs)\n",
    "    np.random.shuffle(valid_idxs)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idxs)\n",
    "valid_sampler = SubsetRandomSampler(valid_idxs)\n",
    "\n",
    "# get data_loaders\n",
    "train_loader = DataLoader(train_set, \n",
    "                      batch_size=batch_size,\n",
    "                      sampler=train_sampler\n",
    "                      )\n",
    "valid_loader = DataLoader(valid_set,\n",
    "                        batch_size=batch_size,\n",
    "                        sampler=valid_sampler\n",
    "                        )\n",
    "test_loader = DataLoader(test_set,\n",
    "                        batch_size=len(test_set)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_font_size = 128*128, img_dim=1, disc_dim = 64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = conv2d(img_dim, disc_dim, k_size=5, stride=2, pad=2, dilation=2, lrelu=False, bn=False)\n",
    "        self.conv2 = conv2d(disc_dim, disc_dim*2, k_size=5, stride=4, pad=2, dilation=2)\n",
    "        self.conv3 = conv2d(disc_dim*2, disc_dim*4, k_size=4, stride=4, pad=1, dilation=1)\n",
    "        self.conv4 = conv2d(disc_dim*4, disc_dim*8)\n",
    "        self.conv5 = conv2d(disc_dim*8, disc_dim*8)\n",
    "        self.fc1 = nn.Linear(disc_dim*8 , 1)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        batch_size = images.shape[0]\n",
    "        images = images.unsqueeze(dim=1)\n",
    "        print(images.shape) # [8, 1, 128, 128]\n",
    "        h1 = self.conv1(images)\n",
    "        print(h1.shape) # [8, 64, 64, 64]\n",
    "        h2 = self.conv2(h1)\n",
    "        print(h2.shape) # [8, 128, 16, 16]\n",
    "        h3 = self.conv3(h2)\n",
    "        print(h3.shape) # [8, 256, 4, 4]\n",
    "        h4 = self.conv4(h3)        \n",
    "        print(h4.shape) # [8, 512, 2, 2]\n",
    "        h5 = self.conv5(h4)        \n",
    "        print(h5.shape)\n",
    "        out = self.fc1(h5.reshape(batch_size, -1))\n",
    "        print(out.shape)\n",
    "        \n",
    "        return out.squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.layers import Encoder_base, Decoder_base\n",
    "from src.models.layers import Encoder_conv, Decoder_conv\n",
    "#from src.models.layers import Encoder_category, Decoder_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#font_size = 128*128, doc2vec_size = 20, letter_vec_size =1\n",
    "\n",
    "class AE_conv(nn.Module):\n",
    "    def __init__(self, font_size=128*128, doc2vec_size =20, letter_vec_size =1):\n",
    "        super(AE_conv, self).__init__()\n",
    "        #embedded_dim = 64*8 + doc2vec_size + letter_vec_size\n",
    "        \n",
    "        self.Encoder = Encoder_conv(img_dim = 1, conv_dim = 64)\n",
    "        self.Decoder = Decoder_conv(img_dim = 1, embedded_dim = 533, conv_dim =64)\n",
    "        \n",
    "    \n",
    "    def forward(self, x_font, doc2vec_vector, letter_vector):\n",
    "        \n",
    "        origin_shape = x_font.shape\n",
    "        x_font = x_font.view(x_font.shape[0], -1)\n",
    "        \n",
    "        z_latent = self.Encoder(x_font)\n",
    "\n",
    "        z = torch.cat([z_latent, doc2vec_vector, letter_vector], dim =1)\n",
    "        x_hat = self.Decoder(z)\n",
    "        x_hat = x_hat.view(origin_shape)\n",
    "        \n",
    "        return x_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator().cuda()\n",
    "generator = AE_conv().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_criterion = nn.MSELoss().cuda()\n",
    "mse_criterion = nn.BCELoss().cuda()\n",
    "l1_criterion = nn.L1Loss().cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(batch_size, generator, discriminator, images, doc2vec_vector, letter_vector):\n",
    "    d_optimizer.zero_grad()\n",
    "    \n",
    "    #train with the real image (discriminator)\n",
    "    outputs = discriminator(images)\n",
    "    real_loss = F.binary_cross_entropy(outputs, Variable(torch.ones(batch_size)).cuda()) \n",
    "    real_score = outputs \n",
    "        \n",
    "    #train with the real image (generator)\n",
    "    fake_images = generator(images, doc2vec_vector, letter_vector) \n",
    "    outputs = discriminator(fake_images)\n",
    "    fake_loss = F.binary_cross_entropy(outputs, Variable(torch.zeros(batch_size)).cuda()) \n",
    "    fake_score = outputs \n",
    "\n",
    "    d_loss = real_loss + fake_loss \n",
    "    d_loss.backward() \n",
    "    d_optimizer.step()\n",
    "    \n",
    "    return d_loss, real_score, fake_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(batch_size, letter_vector, doc2vec_vector, images, generator, discriminator):\n",
    "    \n",
    "    g_optimizer.zero_grad()\n",
    "   \n",
    "    #get output from generator and discriminator   \n",
    "    fake_images = generator(images, doc2vec_vector, letter_vector) \n",
    "    outputs = discriminator(fake_images)\n",
    "        \n",
    "    g_l1_loss= F.l1_loss(fake_images, images)\n",
    "    g_bce_loss = F.binary_cross_entropy(outputs, Variable(torch.ones(batch_size)).cuda())\n",
    "    \n",
    "    g_loss = g_l1_loss + g_bce_loss\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    return g_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of epochs and initialize figure counter\n",
    "num_epochs = 100\n",
    "num_batches = len(train_loader)\n",
    "num_fig = 0\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for n, (vectors, font) in enumerate(train_loader):\n",
    "                \n",
    "        doc2vec_vector = vectors['font_doc2vec']\n",
    "        letter_vector = vectors['word_index']\n",
    "        \n",
    "        images, letter_vector = font.float().to(device), letter_vector.float().to(device)\n",
    "#        doc2vec_vector = doc2vec_vector.float().to(device)\n",
    " \n",
    "        temp_batch_size = letter_vector.size()[0]\n",
    "    \n",
    "        # Train the discriminator\n",
    "        d_loss, real_score, fake_score =train_discriminator(temp_batch_size, generator, discriminator, images, doc2vec_vector, letter_vector)\n",
    "       \n",
    "        \n",
    "        # Train the generator\n",
    "        g_loss = train_generator(temp_batch_size, letter_vector, doc2vec_vector, images, generator, discriminator)\n",
    "     \n",
    "        if n % 1000 == 0: print(n, \" / \")\n",
    "            \n",
    "        \n",
    "    if epoch % 1 == 0:\n",
    "        print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, g_loss: %.4f, ' \n",
    "              'D(x): %.2f, D(G(z)): %.2f' \n",
    "              %(epoch + 1, num_epochs, n+1, num_batches, d_loss.item(), g_loss.item(),\n",
    "                real_score.data.mean(), fake_score.data.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#savePath = \"D:/tobigs 2019/discriminator_1214.pth\"\n",
    "#torch.save(discriminator.state_dict(), savePath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "#new_model = TestModel()\n",
    "#new_model.load_state_dict(torch.load(\"./output/test_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Variable(torch.randn(52, 16)).cuda()\n",
    "fake_alpha_vector =  Variable(torch.FloatTensor(np.eye(52))).cuda()\n",
    "fake_category_vector =  Variable(torch.FloatTensor(np.eye(5)[np.random.choice(5,52)])).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = generator(z, fake_alpha_vector, fake_category_vector).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = make_grid(images, nrow=52, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.view(images.size(0), 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[3].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 200))\n",
    "for i in range(52):\n",
    "    plt.subplot(50, 2, i+1)\n",
    "    plt.imshow(images[i].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:subin2]",
   "language": "python",
   "name": "conda-env-subin2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
