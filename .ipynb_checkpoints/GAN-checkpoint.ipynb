{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr as py # library to read .Rdata files in python\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix,confusion_matrix,classification_report\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV,KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,f1_score,log_loss,recall_score,classification_report\n",
    "from sklearn.preprocessing import scale, robust_scale, minmax_scale, maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset  \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.model import AE_base\n",
    "from src.data.common.dataset import FontDataset, PickledImageProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "validation_split = .15\n",
    "test_split = .05\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "lr = 0.0001\n",
    "\n",
    "log_interval = 10\n",
    "epochs = 200\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1000 examples\n",
      "processed 2000 examples\n",
      "processed 3000 examples\n",
      "processed 4000 examples\n",
      "processed 5000 examples\n",
      "processed 6000 examples\n",
      "processed 7000 examples\n",
      "processed 8000 examples\n",
      "processed 9000 examples\n",
      "processed 10000 examples\n",
      "processed 11000 examples\n",
      "processed 12000 examples\n",
      "processed 13000 examples\n",
      "processed 14000 examples\n",
      "processed 15000 examples\n",
      "processed 16000 examples\n",
      "processed 17000 examples\n",
      "processed 18000 examples\n",
      "processed 19000 examples\n",
      "processed 20000 examples\n",
      "processed 21000 examples\n",
      "processed 22000 examples\n",
      "processed 23000 examples\n",
      "processed 24000 examples\n",
      "processed 25000 examples\n",
      "processed 26000 examples\n",
      "processed 27000 examples\n",
      "processed 28000 examples\n",
      "processed 29000 examples\n",
      "processed 30000 examples\n",
      "processed 31000 examples\n",
      "processed 32000 examples\n",
      "processed 33000 examples\n",
      "processed 34000 examples\n",
      "processed 35000 examples\n",
      "processed 36000 examples\n",
      "processed 37000 examples\n",
      "processed 38000 examples\n",
      "processed 39000 examples\n",
      "processed 40000 examples\n",
      "processed 41000 examples\n",
      "processed 42000 examples\n",
      "processed 43000 examples\n",
      "processed 44000 examples\n",
      "processed 45000 examples\n",
      "processed 46000 examples\n",
      "processed 47000 examples\n",
      "unpickled total 47320 examples\n",
      "saved total 9464 examples only for byte\n"
     ]
    }
   ],
   "source": [
    "# get Dataset\n",
    "data_dir = 'src/data/dataset/integrated/'\n",
    "serif = PickledImageProvider(data_dir+'train.obj')\n",
    "dataset = FontDataset(serif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9464"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-eb71543599c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "dataset[100][1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get idx samplers\n",
    "dataset_size = len(dataset)\n",
    "idxs = list(range(dataset_size))\n",
    "split_test = int(np.floor(test_split * dataset_size))\n",
    "split_valid = int(np.floor((test_split + validation_split) * dataset_size))\n",
    "\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(idxs)\n",
    "    \n",
    "train_idxs, val_idxs, test_idxs = idxs[split_valid:], idxs[split_test:split_valid], idxs[:split_test]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idxs)\n",
    "valid_sampler = SubsetRandomSampler(val_idxs)\n",
    "test_sampler = SubsetRandomSampler(test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data_loaders\n",
    "train_loader = DataLoader(dataset, \n",
    "                      batch_size=batch_size,\n",
    "                      sampler=train_sampler\n",
    "                      )\n",
    "valid_loader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        sampler=valid_sampler\n",
    "                        )\n",
    "test_loader = DataLoader(dataset,\n",
    "                        batch_size=split_test,\n",
    "                        sampler=test_sampler\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_base(nn.Module):\n",
    "    def __init__(self, \n",
    "                 category_size=5, \n",
    "                 alpha_size=52, \n",
    "                 font_size=256*256,\n",
    "                 z_size=64,\n",
    "                 ):\n",
    "        super(AE_base, self).__init__()\n",
    "        self.Encoder = Encoder_base(input_category_size=category_size,\n",
    "                                    input_alpha_size=alpha_size,\n",
    "                                    input_font_size=font_size,\n",
    "                                    z_size=z_size)\n",
    "        self.Decoder = Decoder_base(z_latent_size=z_size,\n",
    "                                    z_category_size=category_size,\n",
    "                                    z_alpha_size=alpha_size,\n",
    "                                    output_font_size=font_size)\n",
    "    \n",
    "    def forward(self, x_font, alpha_vector, category_vector=None):\n",
    "        origin_shape = x_font.shape\n",
    "        x_font = x_font.view(x_font.shape[0], -1)\n",
    "        alpha_vector = alpha_vector.view(alpha_vector.shape[0], -1)\n",
    "        if category_vector is not None:\n",
    "            category_vector = category_vector.view(category_vector.shape[0], -1)\n",
    "        \n",
    "        if category_vector is not None:\n",
    "            x = torch.cat([x_font, category_vector, alpha_vector], dim=1)\n",
    "        else:\n",
    "            x = torch.cat([x_font, alpha_vector], dim=1)\n",
    "        \n",
    "        z_latent = self.Encoder(x)\n",
    "        \n",
    "        z_latent = z_latent.view(z_latent.shape[0], -1)\n",
    "        if category_vector is not None:    \n",
    "            z = torch.cat([z_latent, category_vector, alpha_vector], dim=1)\n",
    "        else:\n",
    "            z = torch.cat([z_latent, alpha_vector], dim=1)\n",
    "        \n",
    "        x_hat = self.Decoder(z)\n",
    "        x_hat = x_hat.view(origin_shape)\n",
    "        \n",
    "        return x_hat, z_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_category_size=5; input_alpha_size=52; input_font_size=256*256\n",
    "input_size = input_category_size + input_alpha_size + input_font_size\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.model(x.view(x.size(0), input_size))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_font_size=256*256\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, output_font_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 16)\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator().cuda()\n",
    "generator = Generator().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "lr = 0.01\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(discriminator, images, real_labels, fake_images, fake_labels):\n",
    "    discriminator.zero_grad()\n",
    "    outputs = discriminator(images)\n",
    "    real_loss = criterion(outputs, real_labels) #discriminator를 지난 output과 실제 output의 차이\n",
    "    real_score = outputs #맞는걸 discriminator에 넣었을 때 output\n",
    "    \n",
    "    outputs = discriminator(fake_images) #만들어낸 이미지가 들어갔을 떄 \n",
    "    fake_loss = criterion(outputs, fake_labels) #우선 fake_label에는 다 0으로 채워진 엉망인 label이 들어감 \n",
    "    fake_score = outputs #얘는 discriminator가 fake_image에 대해 나름대로 맞춰본 답\n",
    "\n",
    "    d_loss = real_loss + fake_loss #real에 대한 loss와 fake에 대한 loss\n",
    "    d_loss.backward() #둘을 더한 loss를 줄이는 쪽으로 backward\n",
    "    d_optimizer.step()\n",
    "    return d_loss, real_score, fake_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(generator, discriminator_outputs, real_labels):\n",
    "    generator.zero_grad()\n",
    "    g_loss = criterion(discriminator_outputs, real_labels) \n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[8, 65593]' is invalid for input of size 131072",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-33a10eee370f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Train the discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0md_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Sample again from the generator and get output from discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-11ef30fe8b99>\u001b[0m in \u001b[0;36mtrain_discriminator\u001b[1;34m(discriminator, images, real_labels, fake_images, fake_labels)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mreal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#discriminator를 지난 output과 실제 output의 차이\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mreal_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;31m#맞는걸 discriminator에 넣었을 때 output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\subin2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-13c6eb925434>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[8, 65593]' is invalid for input of size 131072"
     ]
    }
   ],
   "source": [
    "# set number of epochs and initialize figure counter\n",
    "num_epochs = 100\n",
    "num_batches = len(train_loader)\n",
    "num_fig = 0\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for n, (_, images) in enumerate(train_loader):\n",
    "        \n",
    "        images = Variable(images.float().cuda())\n",
    "        real_labels = Variable(torch.ones(images.size(0)).cuda())\n",
    "        \n",
    "        \n",
    "        # Sample from generator\n",
    "        noise = Variable(torch.randn(images.size(0), 16).cuda()) #랜덤으로 숫자 100*100개 뽑음 (batchsize)*(크기)\n",
    "        fake_images = generator(noise) #그 noise를 generator에 넣어서 fake data를 만듬\n",
    "        fake_labels = Variable(torch.zeros(images.size(0)).cuda()) #100개(batch개수)를 다 0으로 둠\n",
    "        \n",
    "        # Train the discriminator\n",
    "        d_loss, real_score, fake_score = train_discriminator(discriminator, images, real_labels, fake_images, fake_labels)\n",
    "        \n",
    "        # Sample again from the generator and get output from discriminator\n",
    "        noise = Variable(torch.randn(images.size(0), 16).cuda())#랜덤으로 숫자 100*100개 뽑음 (batchsize)*(크기)\n",
    "        fake_images = generator(noise) #그 noise를 generator에 넣어서 fake data를 만듬\n",
    "        outputs = discriminator(fake_images) #그 fake_images에 대한 output을 discriminator를 통해 뽑음\n",
    "\n",
    "        # Train the generator\n",
    "        g_loss = train_generator(generator, outputs, real_labels)\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, g_loss: %.4f, ' \n",
    "              'D(x): %.2f, D(G(z)): %.2f' \n",
    "              %(epoch + 1, num_epochs, n+1, num_batches, d_loss.item(), g_loss.item(),\n",
    "                real_score.data.mean(), fake_score.data.mean()))\n",
    "    if(epoch>1000 and d_loss.item()<3 and g_loss.item()<3 and real_score.data.mean()>0.7 and fake_score.data.mean()>0.4):\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw samples from the input distribution to inspect the generation on training \n",
    "num_test_samples = 10000\n",
    "test_noise = Variable(torch.randn(num_test_samples, 16).cuda())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:subin2]",
   "language": "python",
   "name": "conda-env-subin2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
